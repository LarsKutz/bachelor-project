{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Storing Data with LangChain and ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from langchain_core.documents import Document\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader\n",
    "from unstructured.partition.docx import partition_docx\n",
    "from unstructured.partition.doc import partition_doc\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "from unstructured.chunking.title import chunk_by_title\n",
    "from unstructured.chunking.basic import chunk_elements\n",
    "from unstructured.documents.elements import Image\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "POPPLER_PATH = os.getenv(\"POPPLER_PATH\")\n",
    "TESSERACT_PATH = os.getenv(\"TESSERACT_PATH\")\n",
    "DATA_PATH = os.getenv(\"DATA_PATH\")                                          # contains 108 files  \n",
    "SUB_DATA_SET_PATH = os.path.join(DATA_PATH, \"aktive_leistungen\", \"ark\")     # contains 17 files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_for_double_file_names(curr_file_paths: list[str], target: str) -> bool:\n",
    "    \"\"\" If same file name exists in the list of file paths, return True, else False. \n",
    "    \"\"\"\n",
    "    if not curr_file_paths:\n",
    "        return False\n",
    "    else:\n",
    "        for file_path in curr_file_paths:\n",
    "            if file_path.split(\"\\\\\")[-1] == target:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "\n",
    "def delete_double_file_path_from_file_path(file_paths: list[str]) -> list[str]:\n",
    "    \"\"\" This function deletes double file paths from a list of file paths.\n",
    "    \"\"\"\n",
    "    unique_file_paths = []\n",
    "    double_file_paths = []\n",
    "    for file_path in file_paths:\n",
    "        if not search_for_double_file_names(unique_file_paths, file_path.split(\"\\\\\")[-1]):\n",
    "            unique_file_paths.append(file_path)\n",
    "        else:\n",
    "            double_file_paths.append(file_path)\n",
    "    \n",
    "    return unique_file_paths, double_file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(111, 108, 108)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ALL_FILE_PATHS = set([str(f) for f in Path(DATA_PATH).rglob(\"*.*\")])\n",
    "RED_FILE_PATHS, DOUBLE_FILE_PATHS = delete_double_file_path_from_file_path(ALL_FILE_PATHS)         # 108 files, delete files with same file name\n",
    "ALL_FILE_NAMES = set([file_path.split(\"\\\\\")[-1] for file_path in ALL_FILE_PATHS])\n",
    "DATABASE_PATH = \"../../Database/\"\n",
    "EMBEDDING_MODEL = \"text-embedding-ada-002\"\n",
    "\n",
    "len(ALL_FILE_PATHS), len(RED_FILE_PATHS), len(ALL_FILE_NAMES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating RCTS Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "CHUNK_SIZE = 1000\n",
    "CHUNK_OVERLAP = int(CHUNK_SIZE * (1/5)) \n",
    "SEPARATORS = [\"\\n{2,}\", \"(?<=[.?!])\\s*\\n|\\n\\s*\", \"[.!?]\"]   # 1. Split by amount of newlines, 2. Split by newlines after punctuation, 3. Split by punctuation\n",
    "IS_SEPARATOR_REGEX = True\n",
    "STRIP_WHITESPACE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creation of splitters\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP,\n",
    "    separators=SEPARATORS,\n",
    "    is_separator_regex=IS_SEPARATOR_REGEX,\n",
    "    strip_whitespace=STRIP_WHITESPACE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = chromadb.PersistentClient(\n",
    "    path=os.path.join(DATABASE_PATH, \"RCTS\", f\"{EMBEDDING_MODEL}\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_already_exists = True\n",
    "\n",
    "for file_path, file_name in zip(ALL_FILE_PATHS, ALL_FILE_NAMES):\n",
    "    \n",
    "    chunks = None\n",
    "    if file_name.endswith(\".pdf\"):\n",
    "        docs = PyPDFLoader(file_path).load()\n",
    "        chunks = splitter.split_documents(docs)\n",
    "    \n",
    "    elif file_name.endswith(\".docx\"):\n",
    "        docs = Docx2txtLoader(file_path).load()\n",
    "        chunks = splitter.split_documents(docs)\n",
    "    \n",
    "    else:\n",
    "        print(f\"File type not supported: {file_name}\")\n",
    "    \n",
    "    \n",
    "    Chroma.from_documents(\n",
    "        documents=chunks,\n",
    "        embedding=OpenAIEmbeddings(api_key=OPENAI_API_KEY, model=EMBEDDING_MODEL),\n",
    "        client=client,\n",
    "        collection_name=f\"collection_{CHUNK_SIZE}\",\n",
    "        collection_metadata={\n",
    "            \"hnsw:space\": \"cosine\",\n",
    "            \"chunk_size\": CHUNK_SIZE,\n",
    "            \"chunk_overlap\": CHUNK_OVERLAP,\n",
    "            \"separators\": str(SEPARATORS),\n",
    "            \"is_separator_regex\": IS_SEPARATOR_REGEX,\n",
    "            \"strip_whitespace\": STRIP_WHITESPACE,\n",
    "        } if not collection_already_exists else None,   # because metadata is already set and i can't change space or overwrite it\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Unstructured Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "PAR_STRATEGY = \"hi_res\"\n",
    "PAR_LANGUAGES = [\"deu\"]\n",
    "REMOVABLE_ELEMENTS = (Image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_elements(file_path: str, delete_element_types=(Image)) -> list:\n",
    "    \n",
    "    elements = None\n",
    "    if file_path.endswith(\".pdf\"):\n",
    "        elements = partition_pdf(\n",
    "            filename=file_path,\n",
    "            strategy=PAR_STRATEGY,\n",
    "            languages=PAR_LANGUAGES,\n",
    "        )\n",
    "    elif file_path.endswith(\".docx\"):\n",
    "        elements = partition_docx(\n",
    "            filename=file_path,\n",
    "            strategy=PAR_STRATEGY,\n",
    "            languages=PAR_LANGUAGES,\n",
    "        )\n",
    "    elif file_path.endswith(\".doc\"):\n",
    "        elements = partition_doc(\n",
    "            filename=file_path,\n",
    "            strategy=PAR_STRATEGY,\n",
    "            languages=PAR_LANGUAGES,\n",
    "        )\n",
    "    else:\n",
    "        print(f\"File type not supported: {file_path}\")\n",
    "    \n",
    "    if delete_element_types:\n",
    "        elements = [element for element in elements if not isinstance(element, delete_element_types)]\n",
    "    \n",
    "    return elements\n",
    "\n",
    "\n",
    "def create_documents(chunk_elements) -> list:\n",
    "    documents = []\n",
    "    \n",
    "    for chunk_element in chunk_elements:\n",
    "        source = os.path.join(chunk_element.metadata.file_directory, chunk_element.metadata.filename)\n",
    "        page_number = chunk_element.metadata.page_number if chunk_element.metadata.page_number != None else -1 \n",
    "\n",
    "        document = Document(\n",
    "            page_content=chunk_element.text,\n",
    "            metadata={\n",
    "                \"source\": source,\n",
    "                \"page_number\": page_number,\n",
    "            },\n",
    "        )\n",
    "        documents.append(document)\n",
    "    \n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 108/108 [36:31<00:00, 20.29s/it, Processing: ark_045_-_einstiegsqualifizierung.pdf]                                 \n"
     ]
    }
   ],
   "source": [
    "elements_dict = defaultdict(dict)\n",
    "\n",
    "with tqdm(RED_FILE_PATHS) as iterator:\n",
    "    for file_path in iterator:\n",
    "        file_name = file_path.split(\"\\\\\")[-1]\n",
    "        iterator.set_postfix_str(f\"Processing: {file_name}\")\n",
    "        \n",
    "        elements = get_elements(file_path, delete_element_types=REMOVABLE_ELEMENTS)\n",
    "        \n",
    "        elements_dict[file_name][\"elements\"] = elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `basic`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2500, 1500, 300)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parameters\n",
    "\n",
    "CHUNK_SIZE = 1500\n",
    "OVERLAP = int(CHUNK_SIZE * (1/5))\n",
    "MAX_CHARACTERS = int(CHUNK_SIZE * (5/3))    # chunk size + 2/3 chunk size\n",
    "METHOD = \"basic\"\n",
    "DB_PATH = os.path.join(DATABASE_PATH, \"Unstructured\", METHOD, f\"{EMBEDDING_MODEL}\")\n",
    "\n",
    "CLIENT = chromadb.PersistentClient(\n",
    "    path=DB_PATH,\n",
    ")\n",
    "\n",
    "MAX_CHARACTERS, CHUNK_SIZE, OVERLAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 108/108 [00:00<00:00, 545.04it/s, Processing: ark_045_-_einstiegsqualifizierung.pdf]                                 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1583"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_documents = []\n",
    "\n",
    "with tqdm(elements_dict.keys()) as iterator:\n",
    "    for file_path in iterator:\n",
    "        file_name = file_path.split(\"\\\\\")[-1]\n",
    "        iterator.set_postfix_str(f\"Processing: {file_name}\")\n",
    "        \n",
    "        elements = elements_dict[file_name][\"elements\"]\n",
    "        \n",
    "        chunks = chunk_elements(\n",
    "            elements=elements,\n",
    "            max_characters=MAX_CHARACTERS,\n",
    "            new_after_n_chars=CHUNK_SIZE,\n",
    "            overlap=OVERLAP,\n",
    "            overlap_all=True,\n",
    "        )\n",
    "        \n",
    "        documents = create_documents(chunks)\n",
    "        all_documents.extend(documents)\n",
    "\n",
    "len(all_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_chroma.vectorstores.Chroma at 0x2964f1a0110>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# collection_already_exists = False\n",
    "\n",
    "# Chroma.from_documents(\n",
    "#     documents=all_documents,\n",
    "#     embedding=OpenAIEmbeddings(api_key=OPENAI_API_KEY, model=EMBEDDING_MODEL),\n",
    "#     client=CLIENT,\n",
    "#     collection_name=f\"collection_{CHUNK_SIZE}\",\n",
    "#     collection_metadata={\n",
    "#         \"hnsw:space\": \"cosine\",\n",
    "#         \"chunk_size\": CHUNK_SIZE,\n",
    "#         \"max_characters\": MAX_CHARACTERS,\n",
    "#         \"chunk_overlap\": OVERLAP,\n",
    "#         \"method\": METHOD,\n",
    "#         \"embedding_model\": EMBEDDING_MODEL,\n",
    "#     } if not collection_already_exists else None,   # because metadata is already set and i can't change space or overwrite it\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `by_title`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1800, 3000, 1200)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parameters\n",
    "\n",
    "CHUNK_SIZE = 1800\n",
    "MAX_CHARACTERS = int(CHUNK_SIZE * (5/3))    # chunk size + 2/3 chunk size\n",
    "COMBINE_TEXT_UNDER_N_CHARS = int(2/3 * CHUNK_SIZE)\n",
    "METHOD = \"by_title\"\n",
    "DB_PATH = os.path.join(DATABASE_PATH, \"Unstructured\", METHOD, f\"{EMBEDDING_MODEL}\")\n",
    "\n",
    "CLIENT = chromadb.PersistentClient(\n",
    "    path=DB_PATH,\n",
    ")\n",
    "\n",
    "CHUNK_SIZE, MAX_CHARACTERS, COMBINE_TEXT_UNDER_N_CHARS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 108/108 [00:00<00:00, 476.70it/s, Processing: ark_045_-_einstiegsqualifizierung.pdf]                                 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1197"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_documents = []\n",
    "\n",
    "with tqdm(RED_FILE_PATHS[:]) as iterator:\n",
    "    for file_path in iterator:\n",
    "        file_name = file_path.split(\"\\\\\")[-1]\n",
    "        iterator.set_postfix_str(f\"Processing: {file_name}\")\n",
    "        \n",
    "        elements = elements_dict[file_name][\"elements\"]\n",
    "        \n",
    "        chunks = chunk_by_title(\n",
    "            elements=elements,\n",
    "            max_characters=MAX_CHARACTERS,\n",
    "            new_after_n_chars=CHUNK_SIZE,\n",
    "            combine_text_under_n_chars=COMBINE_TEXT_UNDER_N_CHARS,\n",
    "        )\n",
    "        \n",
    "        documents = create_documents(chunks)\n",
    "        all_documents.extend(documents)\n",
    "\n",
    "len(all_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_chroma.vectorstores.Chroma at 0x2964f0d4190>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# collection_already_exists = False\n",
    "\n",
    "# Chroma.from_documents(\n",
    "#     documents=all_documents,\n",
    "#     embedding=OpenAIEmbeddings(api_key=OPENAI_API_KEY, model=EMBEDDING_MODEL),\n",
    "#     client=CLIENT,\n",
    "#     collection_name=f\"collection_{CHUNK_SIZE}\",\n",
    "#     collection_metadata={\n",
    "#         \"hnsw:space\": \"cosine\",\n",
    "#         \"chunk_size\": CHUNK_SIZE,\n",
    "#         \"max_characters\": MAX_CHARACTERS,\n",
    "#         \"combine_text_under_n_chars\": COMBINE_TEXT_UNDER_N_CHARS,\n",
    "#         \"method\": METHOD,\n",
    "#         \"embedding_model\": EMBEDDING_MODEL, \n",
    "#     } if not collection_already_exists else None,   # because metadata is already set and i can't change space or overwrite it\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Project-KdxDjv4v",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
