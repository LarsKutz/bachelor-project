{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import chromadb\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from flashrank import Ranker\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_cohere import CohereRerank\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain.retrievers.document_compressors.flashrank_rerank import FlashrankRerank\n",
    "from langchain.retrievers.contextual_compression import ContextualCompressionRetriever\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "DATABASE_PATH = os.getenv('DATABASE_PATH')\n",
    "COHERE_API_KEY = os.getenv('COHERE_API_KEY')\n",
    "\n",
    "EMBEDDING_MODEL = 'text-embedding-ada-002'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading questions\n",
    "\n",
    "questions = json.load(open(\"../../Source/Questions/questions_seed_1.json\"))\n",
    "question = questions[0][\"question\"]\n",
    "question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_output_text(text: str, words_per_line: int = 10) -> str:\n",
    "    text_parts = text.split('\\n')\n",
    "    pretty_text = ''\n",
    "    \n",
    "    for text_part in text_parts:\n",
    "        words = text_part.split(' ')\n",
    "        for i, word in enumerate(words):\n",
    "            pretty_text += word + ' '\n",
    "            if (i + 1) % words_per_line == 0 and i != len(words) - 1:\n",
    "                pretty_text += '\\n'\n",
    "        pretty_text += '\\n'\n",
    "    \n",
    "    return pretty_text\n",
    "\n",
    "\n",
    "def get_privat_path(source: str) -> str:\n",
    "    parts = source.split('\\\\')[8:]\n",
    "    return os.path.join(*parts)\n",
    "\n",
    "\n",
    "def pretty_output_llm_answer(chain_output: dict, line_len: int=1_000_000, print_privat_path=True) -> str:\n",
    "    print(f\"Question: {question}\")\n",
    "    print(\"-\" * 150, end='\\n\\n')\n",
    "    \n",
    "    sources = None\n",
    "    if not print_privat_path:\n",
    "        sources = \" \\n\".join(f\"{doc.metadata['source']} - {doc.metadata['page_number']}\" for doc in chain_output[\"context\"])\n",
    "    else:\n",
    "        sources = \" \\n\".join(f\"{get_privat_path(doc.metadata['source'])}\" for doc in chain_output[\"context\"])\n",
    "    \n",
    "    full_answer = f\"{pretty_output_text(chain_output['answer'], line_len)}\\n\\nQuellen:\\n{sources}\"\n",
    "    \n",
    "    print(full_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM = ChatOpenAI(\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "PROMPT = ChatPromptTemplate([\n",
    "    (\"system\", \"\"\"Du bist ein Assistant einer öffentlichen Behörde und deine Aufgabe ist es, Fragen nur auf Basis des bereitgestellten Kontexts zu beantworten.\n",
    "\n",
    "- Wenn die Frage anhand des gegebenen Kontexts beantwortet werden kann, beantworte sie unter Einbeziehung relevanter Paragrafen, Gesetze oder Vorschriften, die im Kontext erwähnt werden.\n",
    "- Wenn die Frage im Kontext nicht eindeutig beantwortet werden kann oder keine ausreichenden Informationen vorliegen, gib an, dass du die Frage nicht beantworten kannst.\n",
    "- Achte besonders darauf, dass du keine Informationen hinzufügst, die nicht im Kontext enthalten sind.\n",
    "\n",
    "Am Ende deiner Antwort weise bitte darauf hin, dass du ein ChatBot bist und die Antwort unbedingt von einer qualifizierten Person überprüft werden sollte.\n",
    "\n",
    "<kontext>\n",
    "{context}\n",
    "</kontext>\"\"\"),\n",
    "    (\"human\", \"Frage: {input}\")\n",
    "])\n",
    "\n",
    "\n",
    "def get_vectorstore(type: str):\n",
    "    \"\"\" Create a vectorstore.\n",
    "    \n",
    "    Args:\n",
    "        type (str): \"basic\" or \"by_title\"\n",
    "    \"\"\"\n",
    "    collection_name = None\n",
    "    if type == \"basic\":\n",
    "        collection_name = 'collection_1500'\n",
    "    elif type == \"by_title\":\n",
    "        collection_name = 'collection_1800'\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid type: {type}\")\n",
    "    \n",
    "    db_path = os.path.join(DATABASE_PATH, \"Unstructured\", type, EMBEDDING_MODEL)\n",
    "    db_client = chromadb.PersistentClient(db_path)\n",
    "    \n",
    "    return Chroma(\n",
    "        collection_name=collection_name,\n",
    "        client=db_client,\n",
    "        embedding_function=OpenAIEmbeddings(model=EMBEDDING_MODEL, api_key=OPENAI_API_KEY),\n",
    "        create_collection_if_not_exists=False\n",
    "    )\n",
    "\n",
    "\n",
    "def get_retriever(type: str, k: int, mq: bool=False):\n",
    "    \"\"\" Create a retriever.\n",
    "    \n",
    "    Args:\n",
    "        type (str): \"basic\" or \"by_title\"\n",
    "        k (int): Number of documents to retrieve\n",
    "        mq (bool): Whether to use MultiQueryRetriever\n",
    "    \"\"\"\n",
    "    vectorstore = get_vectorstore(type)\n",
    "    \n",
    "    retriever = vectorstore.as_retriever(\n",
    "        search_type='similarity',\n",
    "        search_kwargs={\n",
    "            'k': k,\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    if not mq:\n",
    "        return retriever\n",
    "    else:\n",
    "        mq_retriever = MultiQueryRetriever.from_llm(\n",
    "            retriever=retriever,\n",
    "            llm=LLM,\n",
    "        )\n",
    "        return mq_retriever\n",
    "\n",
    "\n",
    "def get_reranker(k: int, type: str):\n",
    "    \"\"\" Create a reranker.\n",
    "    \n",
    "    Args:\n",
    "        k (int): Number of documents to rerank\n",
    "        type (str): \"flashrank\" or \"cohere\"\n",
    "    \"\"\"\n",
    "    if type == \"flashrank\":\n",
    "        client = Ranker(\n",
    "            model_name='rank-T5-flan',\n",
    "            max_length=4096,\n",
    "        )\n",
    "        return FlashrankRerank(\n",
    "            client=client,\n",
    "            top_n=k,\n",
    "        )\n",
    "    elif type == \"cohere\":\n",
    "        return CohereRerank(\n",
    "            top_n=k,\n",
    "            model='rerank-multilingual-v3.0',\n",
    "            cohere_api_key=COHERE_API_KEY,\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid type: {type}\")\n",
    "\n",
    "\n",
    "def process(question: str, k: int, type: str, rerank_k: int=3, rerank_type: str=None, mq: bool=False):\n",
    "    \"\"\" Process a question.\n",
    "    \n",
    "    Args:\n",
    "        question (str): Question to process\n",
    "        k (int): Number of documents to retrieve\n",
    "        type (str): \"basic\" or \"by_title\"\n",
    "        rerank_k (int): Number of documents to rerank. Defaults to 3.\n",
    "        rerank_type (str): \"flashrank\" or \"cohere\". Defaults to None.\n",
    "        mq (bool, optional): Whether to use MultiQueryRetriever. Defaults to False.\n",
    "    \"\"\"\n",
    "    \n",
    "    retriever = get_retriever(type, k, mq)\n",
    "    reranker = get_reranker(rerank_k, rerank_type) if rerank_type else None\n",
    "    \n",
    "    if rerank_type:\n",
    "        retriever = ContextualCompressionRetriever(\n",
    "            base_retriever=retriever,\n",
    "            base_compressor=reranker,\n",
    "        )\n",
    "    retrieval_chain = create_retrieval_chain(\n",
    "        retriever=retriever,\n",
    "        combine_docs_chain=create_stuff_documents_chain(\n",
    "            llm=LLM,\n",
    "            prompt=PROMPT,\n",
    "        )\n",
    "    )\n",
    "    return retrieval_chain.invoke({\"input\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_dict = process(question, 5, \"basic\", 2, \"cohere\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty_output_llm_answer(answer_dict, 10, print_privat_path=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Project-KdxDjv4v",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
