{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunks to `.txt` files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is an evaluation of different chunking methods with different parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "DATA_PATH = os.getenv(\"DATA_PATH\")\n",
    "SUB_DATA_SET_PATH = os.path.join(DATA_PATH, \"aktive_leistungen\", \"ark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_file_paths_generator = Path(SUB_DATA_SET_PATH).rglob(\"*.*\")\n",
    "ALL_FILE_PATHS = [str(f) for f in all_file_paths_generator]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1000, 200, ['\\n{2,}', '(?<=[.?!])\\\\s*\\n|\\n\\\\s*', '[.!?]']),\n",
       " (1500, 350, ['\\n{2,}', '(?<=[.?!])\\\\s*\\n|\\n\\\\s*', '[.!?]']),\n",
       " (2000, 500, ['\\n{2,}', '(?<=[.?!])\\\\s*\\n|\\n\\\\s*', '[.!?]'])]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SAVING_PATH = \"Data/Chunks/RecursiveCharacterTextSplitter/\"\n",
    "CHUNK_SIZES = [1000, 1500, 2000]\n",
    "CHUNK_OVERLAPS = [200, 350, 500]\n",
    "separators = [\"\\n{2,}\", \"(?<=[.?!])\\s*\\n|\\n\\s*\", \"[.!?]\"]\n",
    "\n",
    "rcts_parameters = [(chunk_size, chunk_overlap, separators) for chunk_size, chunk_overlap in zip(CHUNK_SIZES, CHUNK_OVERLAPS)]\n",
    "rcts_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/17 [00:00<?, ?it/s, Processing aktive_leistungen_bei_darlehensweiser_passiver_leistungsgewaehrung]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:03<00:00,  5.50it/s, Processing zahlungsmodalitaeten]                                     \n"
     ]
    }
   ],
   "source": [
    "with tqdm(ALL_FILE_PATHS) as iterator:\n",
    "    for file_path in iterator:\n",
    "        file_name = file_path.split(\"\\\\\")[-1].split(\".\")[0]     # file name without extension\n",
    "        iterator.set_postfix_str(f\"Processing {file_name}\")\n",
    "        \n",
    "        # Loading the document\n",
    "        docs = None\n",
    "        if file_path.endswith(\".pdf\"):\n",
    "            docs = PyPDFLoader(file_path).load()\n",
    "        elif file_path.endswith(\".docx\"):\n",
    "            docs = Docx2txtLoader(file_path).load()\n",
    "        \n",
    "        for paramters in rcts_parameters:\n",
    "            splitter = RecursiveCharacterTextSplitter(\n",
    "                chunk_size=paramters[0],\n",
    "                chunk_overlap=paramters[1],\n",
    "                separators=paramters[2],\n",
    "                is_separator_regex=True,\n",
    "                strip_whitespace=True,\n",
    "            )   \n",
    "            \n",
    "            chunks = splitter.split_documents(docs)\n",
    "            \n",
    "            # Saving the chunks\n",
    "            chunk_path = os.path.join(SAVING_PATH, str(paramters[0]), str(paramters[0])+\"_\"+file_name+\".txt\")\n",
    "            with open(chunk_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(\"=\" * 150)\n",
    "                f.write(\"\\n\")\n",
    "                f.write(f\"file:\\t\\t\\t{file_path}\\n\")\n",
    "                f.write(f\"chunk_size:\\t\\t{paramters[0]}\\n\")\n",
    "                f.write(f\"chunk_overlap:\\t{paramters[1]}\\n\")\n",
    "                f.write(f\"separators:\\t\\t{paramters[2]}\\n\")\n",
    "                f.write(f\"n_chunks:\\t\\t{len(chunks)}\\n\")\n",
    "                f.write(\"=\" * 150)\n",
    "                f.write(\"\\n\\n\\n\\n\")\n",
    "                for i, chunk in enumerate(chunks, 1):\n",
    "                    f.write(f\"Chunk #{i}\\n\")\n",
    "                    f.write(\"-\" * 150)\n",
    "                    f.write(\"\\n\")\n",
    "                    f.write(chunk.page_content)\n",
    "                    if i != len(chunks):\n",
    "                        f.write(\"\\n\\n\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unstructured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "from unstructured.chunking.basic import chunk_elements\n",
    "from unstructured.chunking.title import chunk_by_title\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "from unstructured.partition.docx import partition_docx\n",
    "from unstructured.partition.doc import partition_doc\n",
    "from unstructured.documents.elements import Image\n",
    "\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "POPPLER_PATH = os.getenv(\"POPPLER_PATH\")\n",
    "TESSERACT_PATH = os.getenv(\"TESSERACT_PATH\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVING_PATH = \"Data/Chunks/\"\n",
    "CHUNK_SIZES = [1000, 1500, 2000, 3000]\n",
    "CHUNKING_STRATEGY = [\"basic\", \"by_title\"]\n",
    "MODES = [\"naive\", \"advanced\"]\n",
    "MAX_CHARACTERS = [i+1000 for i in CHUNK_SIZES]  # only necessary in 2nd round\n",
    "COMBINE_TEXT_UNDER_N_CHARS = [int(i*2/3) for i in CHUNK_SIZES]  # only necessary in 2nd round\n",
    "STRATEGY = \"hi_res\"\n",
    "LANGUAGES = [\"deu\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def part_pdf(file_path):\n",
    "    return partition_pdf(\n",
    "        filename=file_path,\n",
    "        strategy=STRATEGY,\n",
    "        languages=LANGUAGES,\n",
    "    )\n",
    "\n",
    "def part_docx(file_path):\n",
    "    return partition_docx(\n",
    "        filename=file_path,\n",
    "        strategy=STRATEGY,\n",
    "        languages=LANGUAGES,\n",
    "    )\n",
    "\n",
    "def part_doc(file_path):\n",
    "    return partition_doc(\n",
    "        filename=file_path,\n",
    "        strategy=STRATEGY,\n",
    "        languages=LANGUAGES,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Basic Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "OVERLAP = [int(i/5) for i in CHUNK_SIZES]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [04:26<00:00, 15.66s/it, Processing zahlungsmodalitaeten.pdf - 3000]                                      \n"
     ]
    }
   ],
   "source": [
    "# naive\n",
    "chunking_strategy = CHUNKING_STRATEGY[0].title()\n",
    "mode = MODES[0].title()\n",
    "\n",
    "with tqdm(ALL_FILE_PATHS) as iterator:\n",
    "    for file_path in iterator:\n",
    "        file_name = file_path.split(\"\\\\\")[-1]\n",
    "        iterator.set_postfix_str(f\"Processing {file_name}\")\n",
    "        \n",
    "        elements = None\n",
    "        if file_name.endswith(\".pdf\"):\n",
    "            elements = part_pdf(file_path)\n",
    "        elif file_name.endswith(\".docx\"):\n",
    "            elements = part_docx(file_path)\n",
    "        elif file_name.endswith(\".doc\"):\n",
    "            elements = part_doc(file_path)\n",
    "        reduced_elements = [element for element in elements if not isinstance(element, (Image))]\n",
    "        \n",
    "        for chunk_size, overlap in zip(CHUNK_SIZES, OVERLAP):\n",
    "            iterator.set_postfix_str(f\"Processing {file_name} - {chunk_size}\")\n",
    "            \n",
    "            chunks = chunk_elements(\n",
    "                elements=reduced_elements,\n",
    "                max_characters=chunk_size,\n",
    "                overlap=overlap,\n",
    "                overlap_all=True,\n",
    "            )\n",
    "            \n",
    "            # Saving the chunks\n",
    "            chunk_path = os.path.join(SAVING_PATH, chunking_strategy, mode, str(chunk_size))\n",
    "            if not os.path.exists(chunk_path):\n",
    "                os.makedirs(chunk_path)\n",
    "            \n",
    "            with open(os.path.join(chunk_path, str(chunk_size)+\"_\"+file_name+\".txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(\"=\" * 150)\n",
    "                f.write(\"\\n\")\n",
    "                f.write(f\"file:\\t\\t\\t\\t{file_path}\\n\")\n",
    "                f.write(f\"chunk_size:\\t\\t\\t{chunk_size}\\n\")\n",
    "                f.write(f\"overlap:\\t\\t\\t{overlap}\\n\")\n",
    "                f.write(f\"loading_strategy:\\t{STRATEGY}\\n\")\n",
    "                f.write(f\"chunking_strategy:\\t{chunking_strategy}\\n\")\n",
    "                f.write(f\"mode:\\t\\t\\t\\t{mode}\\n\")\n",
    "                f.write(f\"n_chunks:\\t\\t\\t{len(chunks)}\\n\")\n",
    "                f.write(\"=\" * 150)\n",
    "                f.write(\"\\n\\n\\n\\n\")\n",
    "                for i, chunk in enumerate(chunks, 1):\n",
    "                    f.write(f\"Chunk #{i}\\n\")\n",
    "                    f.write(\"-\" * 150)\n",
    "                    f.write(\"\\n\")\n",
    "                    f.write(chunk.text)\n",
    "                    if i != len(chunks):\n",
    "                        f.write(\"\\n\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [05:27<00:00, 19.27s/it, Processing zahlungsmodalitaeten.pdf - 3000]                                      \n"
     ]
    }
   ],
   "source": [
    "# advanced\n",
    "chunking_strategy = CHUNKING_STRATEGY[0].title()\n",
    "mode = MODES[1].title()\n",
    "\n",
    "with tqdm(ALL_FILE_PATHS) as iterator:\n",
    "    for file_path in iterator:\n",
    "        file_name = file_path.split(\"\\\\\")[-1]\n",
    "        iterator.set_postfix_str(f\"Processing {file_name}\")\n",
    "        \n",
    "        elements = None\n",
    "        if file_name.endswith(\".pdf\"):\n",
    "            elements = part_pdf(file_path)\n",
    "        elif file_name.endswith(\".docx\"):\n",
    "            elements = part_docx(file_path)\n",
    "        elif file_name.endswith(\".doc\"):\n",
    "            elements = part_doc(file_path)\n",
    "        reduced_elements = [element for element in elements if not isinstance(element, (Image))]\n",
    "        \n",
    "        for max_characters, chunk_size, overlap in zip(MAX_CHARACTERS, CHUNK_SIZES, OVERLAP):\n",
    "            iterator.set_postfix_str(f\"Processing {file_name} - {chunk_size}\")\n",
    "            \n",
    "            chunks = chunk_elements(\n",
    "                elements=reduced_elements,\n",
    "                max_characters=max_characters,\n",
    "                new_after_n_chars=chunk_size,\n",
    "                overlap=overlap,\n",
    "                overlap_all=True,\n",
    "            )\n",
    "            \n",
    "            # Saving the chunks\n",
    "            chunk_path = os.path.join(SAVING_PATH, chunking_strategy, mode, str(chunk_size))\n",
    "            if not os.path.exists(chunk_path):\n",
    "                os.makedirs(chunk_path)\n",
    "            \n",
    "            with open(os.path.join(chunk_path, str(chunk_size)+\"_\"+file_name+\".txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(\"=\" * 150)\n",
    "                f.write(\"\\n\")\n",
    "                f.write(f\"file:\\t\\t\\t\\t{file_path}\\n\")\n",
    "                f.write(f\"max_characters:\\t\\t{max_characters}\\n\")\n",
    "                f.write(f\"aimed_chunk_size:\\t{chunk_size}\\n\")\n",
    "                f.write(f\"overlap:\\t\\t\\t{overlap}\\n\")\n",
    "                f.write(f\"loading_strategy:\\t{STRATEGY}\\n\")\n",
    "                f.write(f\"chunking_strategy:\\t{chunking_strategy}\\n\")\n",
    "                f.write(f\"mode:\\t\\t\\t\\t{mode}\\n\")\n",
    "                f.write(f\"n_chunks:\\t\\t\\t{len(chunks)}\\n\")\n",
    "                f.write(\"=\" * 150)\n",
    "                f.write(\"\\n\\n\\n\\n\")\n",
    "                for i, chunk in enumerate(chunks, 1):\n",
    "                    f.write(f\"Chunk #{i}\\n\")\n",
    "                    f.write(\"-\" * 150)\n",
    "                    f.write(\"\\n\")\n",
    "                    f.write(chunk.text)\n",
    "                    if i != len(chunks):\n",
    "                        f.write(\"\\n\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Chunking by Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [06:43<00:00, 23.73s/it, Processing zahlungsmodalitaeten.pdf - 3000]                                              \n"
     ]
    }
   ],
   "source": [
    "# naive\n",
    "chunking_strategy = CHUNKING_STRATEGY[1].title()\n",
    "mode = MODES[0].title()\n",
    "\n",
    "with tqdm(ALL_FILE_PATHS) as iterator:\n",
    "    for file_path in iterator:\n",
    "        file_name = file_path.split(\"\\\\\")[-1]\n",
    "        iterator.set_postfix_str(f\"Processing {file_name}\")\n",
    "        \n",
    "        elements = None\n",
    "        if file_name.endswith(\".pdf\"):\n",
    "            elements = part_pdf(file_path)\n",
    "        elif file_name.endswith(\".docx\"):\n",
    "            elements = part_docx(file_path)\n",
    "        elif file_name.endswith(\".doc\"):\n",
    "            elements = part_doc(file_path)\n",
    "        reduced_elements = [element for element in elements if not isinstance(element, (Image))]\n",
    "        \n",
    "        for chunk_size in CHUNK_SIZES:\n",
    "            iterator.set_postfix_str(f\"Processing {file_name} - {chunk_size}\")\n",
    "            \n",
    "            chunks = chunk_by_title(\n",
    "                elements=reduced_elements,\n",
    "                max_characters=chunk_size,\n",
    "            )\n",
    "            \n",
    "            # Saving the chunks\n",
    "            chunk_path = os.path.join(SAVING_PATH, chunking_strategy, mode, str(chunk_size))\n",
    "            if not os.path.exists(chunk_path):\n",
    "                os.makedirs(chunk_path)\n",
    "            \n",
    "            with open(os.path.join(chunk_path, str(chunk_size)+\"_\"+file_name+\".txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(\"=\" * 150)\n",
    "                f.write(\"\\n\")\n",
    "                f.write(f\"file:\\t\\t\\t\\t{file_path}\\n\")\n",
    "                f.write(f\"chunk_size:\\t\\t\\t{chunk_size}\\n\")\n",
    "                f.write(f\"loading_strategy:\\t{STRATEGY}\\n\")\n",
    "                f.write(f\"chunking_strategy:\\t{chunking_strategy}\\n\")\n",
    "                f.write(f\"mode:\\t\\t\\t\\t{mode}\\n\")\n",
    "                f.write(f\"n_chunks:\\t\\t\\t{len(chunks)}\\n\")\n",
    "                f.write(\"=\" * 150)\n",
    "                f.write(\"\\n\\n\\n\\n\")\n",
    "                for i, chunk in enumerate(chunks, 1):\n",
    "                    f.write(f\"Chunk #{i}\\n\")\n",
    "                    f.write(\"-\" * 150)\n",
    "                    f.write(\"\\n\")\n",
    "                    f.write(chunk.text)\n",
    "                    if i != len(chunks):\n",
    "                        f.write(\"\\n\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [06:36<00:00, 23.35s/it, Processing zahlungsmodalitaeten.pdf - 3000]                                      \n"
     ]
    }
   ],
   "source": [
    "# naive\n",
    "chunking_strategy = CHUNKING_STRATEGY[1].title()\n",
    "mode = MODES[1].title()\n",
    "\n",
    "with tqdm(ALL_FILE_PATHS) as iterator:\n",
    "    for file_path in iterator:\n",
    "        file_name = file_path.split(\"\\\\\")[-1]\n",
    "        iterator.set_postfix_str(f\"Processing {file_name}\")\n",
    "        \n",
    "        elements = None\n",
    "        if file_name.endswith(\".pdf\"):\n",
    "            elements = part_pdf(file_path)\n",
    "        elif file_name.endswith(\".docx\"):\n",
    "            elements = part_docx(file_path)\n",
    "        elif file_name.endswith(\".doc\"):\n",
    "            elements = part_doc(file_path)\n",
    "        reduced_elements = [element for element in elements if not isinstance(element, (Image))]\n",
    "        \n",
    "        for max_characters, chunk_size, combine_under in zip(MAX_CHARACTERS, CHUNK_SIZES, COMBINE_TEXT_UNDER_N_CHARS):\n",
    "            iterator.set_postfix_str(f\"Processing {file_name} - {chunk_size}\")\n",
    "            \n",
    "            chunks = chunk_by_title(\n",
    "                elements=reduced_elements,\n",
    "                max_characters=max_characters,\n",
    "                new_after_n_chars=chunk_size,\n",
    "                combine_text_under_n_chars=combine_under,\n",
    "            )\n",
    "            \n",
    "            # Saving the chunks\n",
    "            chunk_path = os.path.join(SAVING_PATH, chunking_strategy, mode, str(chunk_size))\n",
    "            if not os.path.exists(chunk_path):\n",
    "                os.makedirs(chunk_path)\n",
    "            \n",
    "            with open(os.path.join(chunk_path, str(chunk_size)+\"_\"+file_name+\".txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(\"=\" * 150)\n",
    "                f.write(\"\\n\")\n",
    "                f.write(f\"file:\\t\\t\\t\\t{file_path}\\n\")\n",
    "                f.write(f\"max_characters:\\t\\t{max_characters}\\n\")\n",
    "                f.write(f\"aimed_chunk_size:\\t{chunk_size}\\n\")\n",
    "                f.write(f\"combine_under:\\t\\t{combine_under}\\n\")\n",
    "                f.write(f\"loading_strategy:\\t{STRATEGY}\\n\")\n",
    "                f.write(f\"chunking_strategy:\\t{chunking_strategy}\\n\")\n",
    "                f.write(f\"mode:\\t\\t\\t\\t{mode}\\n\")\n",
    "                f.write(f\"n_chunks:\\t\\t\\t{len(chunks)}\\n\")\n",
    "                f.write(\"=\" * 150)\n",
    "                f.write(\"\\n\\n\\n\\n\")\n",
    "                for i, chunk in enumerate(chunks, 1):\n",
    "                    f.write(f\"Chunk #{i}\\n\")\n",
    "                    f.write(\"-\" * 150)\n",
    "                    f.write(\"\\n\")\n",
    "                    f.write(chunk.text)\n",
    "                    if i != len(chunks):\n",
    "                        f.write(\"\\n\\n\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Project-KdxDjv4v",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
