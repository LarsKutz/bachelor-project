{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FlashRerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import chromadb\n",
    "from flashrank import Ranker, RerankRequest\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.retrievers.contextual_compression import ContextualCompressionRetriever\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.retrievers.document_compressors.flashrank_rerank import FlashrankRerank\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "DATABASE_PATH = os.getenv('DATABASE_PATH')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_MODEL = 'text-embedding-ada-002'\n",
    "FLASHRANK_RERANK_MODEL = 'rank-T5-flan'\n",
    "\n",
    "database_path_basic = os.path.join(DATABASE_PATH, 'Unstructured', 'basic', f\"{EMBEDDING_MODEL}\")\n",
    "database_path_title = os.path.join(DATABASE_PATH, 'Unstructured', 'by_title', f\"{EMBEDDING_MODEL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY = \"Wie kann man eine Auskunftspflicht in einer Haushaltsgemeinschaft durchsetzen?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_output_text(text: str, words_per_line: int = 10) -> str:\n",
    "    text_parts = text.split('\\n')\n",
    "    pretty_text = ''\n",
    "    \n",
    "    for text_part in text_parts:\n",
    "        words = text_part.split(' ')\n",
    "        for i, word in enumerate(words):\n",
    "            pretty_text += word + ' '\n",
    "            if (i + 1) % words_per_line == 0 and i != len(words) - 1:\n",
    "                pretty_text += '\\n'\n",
    "        pretty_text += '\\n'\n",
    "    \n",
    "    return pretty_text\n",
    "\n",
    "\n",
    "def pretty_output_docs(docs: list, show_metadata=True, show_full_path=True) -> str:\n",
    "    print(f\"QUERY: {QUERY}\")\n",
    "    print('*' * 150, end='\\n\\n')\n",
    "    for i, doc in enumerate(docs):\n",
    "        print(f\"CHUNK #{i+1}:\")\n",
    "        if show_metadata:\n",
    "            source_path = doc.metadata['source'] if show_full_path else os.path.basename(doc.metadata['source'])\n",
    "            print(f\"Source:\\t\\t\\t{source_path}\")\n",
    "            print(f\"Page Number:\\t\\t{doc.metadata['page_number']}\")\n",
    "            print(f\"Idx in Retrieving:\\t{doc.metadata['id']}\")\n",
    "            print(f\"Relevance Score:\\t{doc.metadata['relevance_score']}\")\n",
    "        \n",
    "        print('-' * 150)\n",
    "        print(pretty_output_text(doc.page_content, 12))\n",
    "        print('=' * 150)\n",
    "\n",
    "\n",
    "def pretty_output_flashrank_results(flashrank_results: list, show_additional_info=True, show_full_path=True) -> str:\n",
    "    print(f\"QUERY: {QUERY}\")\n",
    "    print('*' * 150, end='\\n\\n')\n",
    "    for i, result in enumerate(flashrank_results):\n",
    "        print(f\"CHUNK #{i+1}:\")\n",
    "        if show_additional_info:\n",
    "            source_path = result['metadata']['source'] if show_full_path else os.path.basename(result['metadata']['source'])\n",
    "            print(f\"Source:\\t\\t\\t{source_path}\")\n",
    "            print(f\"Page Number:\\t\\t{result['metadata']['page_number']}\")\n",
    "            print(f\"Idx in Retrieving:\\t{result['id']}\")\n",
    "            print(f\"Score:\\t{result['score']}\")\n",
    "        \n",
    "        print('-' * 150)\n",
    "        print(pretty_output_text(result['text'], 12))\n",
    "        print('=' * 150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_client_basic = chromadb.PersistentClient(\n",
    "    path=database_path_basic,\n",
    ")\n",
    "collection_name_basic = 'collection_1500'\n",
    "\n",
    "chroma_client_title = chromadb.PersistentClient(\n",
    "    path=database_path_title,\n",
    ")\n",
    "collection_name_title = 'collection_1800'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore_basic = Chroma(\n",
    "    collection_name=collection_name_basic,\n",
    "    client=chroma_client_basic,\n",
    "    embedding_function=OpenAIEmbeddings(model=EMBEDDING_MODEL, api_key=OPENAI_API_KEY),\n",
    "    create_collection_if_not_exists=False,\n",
    ")\n",
    "\n",
    "vectorstore_title = Chroma(\n",
    "    collection_name=collection_name_title,\n",
    "    client=chroma_client_title,\n",
    "    embedding_function=OpenAIEmbeddings(model=EMBEDDING_MODEL, api_key=OPENAI_API_KEY),\n",
    "    create_collection_if_not_exists=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_retrieved_docs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_retriever_basic = vectorstore_basic.as_retriever(\n",
    "    search_kwargs={\n",
    "        'k': n_retrieved_docs,\n",
    "    }\n",
    ")\n",
    "\n",
    "default_retriever_title = vectorstore_title.as_retriever(\n",
    "    search_kwargs={\n",
    "        'k': n_retrieved_docs,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using LangChain Intergration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `FlashrankRerank()` is using `cross-encoder/ms-marco-MultiBERT-L-12` as default reranker. \n",
    "\n",
    "-  This model is trained on [MS MARCO dataset](https://microsoft.github.io/msmarco/).  \n",
    "\n",
    "- Watch [GitHub](https://github.com/PrithivirajDamodaran/FlashRank) for more details.\n",
    "\n",
    "- Following models are available for reranking:\n",
    "    ```python\n",
    "    # flashrank_rerank.py -> Ranker.py -> Config.py\n",
    "    # Config.py\n",
    "    {\n",
    "        \"ms-marco-TinyBERT-L-2-v2\": \"flashrank-TinyBERT-L-2-v2.onnx\",\n",
    "        \"ms-marco-MiniLM-L-12-v2\": \"flashrank-MiniLM-L-12-v2_Q.onnx\",\n",
    "        \"ms-marco-MultiBERT-L-12\": \"flashrank-MultiBERT-L12_Q.onnx\",\n",
    "        \"rank-T5-flan\": \"flashrank-rankt5_Q.onnx\",\n",
    "        \"ce-esci-MiniLM-L12-v2\": \"flashrank-ce-esci-MiniLM-L12-v2_Q.onnx\",\n",
    "        \"rank_zephyr_7b_v1_full\": \"rank_zephyr_7b_v1_full.Q4_K_M.gguf\",\n",
    "        \"miniReranker_arabic_v1\": \"miniReranker_arabic_v1.onnx\"\n",
    "    }\n",
    "    ```\n",
    "\n",
    "- **Cross-Endoder** models can only process **512 tokens**.\n",
    "\n",
    "- **LLM-based** models can process **8096 tokens**.\n",
    "\n",
    "\n",
    "**From [GitHub](https://github.com/PrithivirajDamodaran/FlashRank):**\n",
    "| Model Name | Description | Size | Notes |\n",
    "|------------|-------------|------|-------|\n",
    "| `ms-marco-TinyBERT-L-2-v2` | - | ~4MB | [Model card](https://huggingface.co/cross-encoder/ms-marco-TinyBERT-L-2) |\n",
    "| `ms-marco-MiniLM-L-12-v2` | - | ~34MB | [Model card](https://huggingface.co/cross-encoder/ms-marco-MiniLM-L-12-v2) |\n",
    "| `rank-T5-flan` | Best non cross-encoder reranker | ~110MB | [Model card](https://huggingface.co/bergum/rank-T5-flan) |\n",
    "| `ms-marco-MultiBERT-L-12` | Multi-lingual, supports 100+ languages | ~150MB | [Supported languages](https://github.com/google-research/bert/blob/master/multilingual.md#list-of-languages) |\n",
    "| `ce-esci-MiniLM-L12-v2` | Fine-tuned on Amazon ESCI dataset | - | [Model card](https://huggingface.co/metarank/ce-esci-MiniLM-L12-v2) |\n",
    "| `rank_zephyr_7b_v1_full` | 4-bit-quantised GGUF | ~4GB | [Model card](https://huggingface.co/castorini/rank_zephyr_7b_v1_full) |\n",
    "| `miniReranker_arabic_v1` | - | - | [Model card](https://huggingface.co/prithivida/miniReranker_arabic_v1) |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "compressor = FlashrankRerank(\n",
    "    top_n=5,\n",
    "    model=FLASHRANK_RERANK_MODEL,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "compressor_retriever_basic = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,\n",
    "    base_retriever=default_retriever_basic,\n",
    ")\n",
    "\n",
    "compressor_retriever_title = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,\n",
    "    base_retriever=default_retriever_title,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "compressed_docs_basic = compressor_retriever_basic.invoke(QUERY)\n",
    "compressed_docs_title = compressor_retriever_title.invoke(QUERY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compressed_docs_basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretty_output_docs(compressed_docs_basic, show_full_path=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compressed_docs_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretty_output_docs(compressed_docs_title, show_full_path=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using FlashRank API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `Ranker()` is using `cross-encoder/ms-marco-TinyBERT-L-2-v2` as default reranker. \n",
    "\n",
    "- Watch [GitHub](https://github.com/PrithivirajDamodaran/FlashRank) for more details.\n",
    "\n",
    "**From [GitHub](https://github.com/PrithivirajDamodaran/FlashRank):**\n",
    "| Model Name | Description | Size | Notes |\n",
    "|------------|-------------|------|-------|\n",
    "| `ms-marco-TinyBERT-L-2-v2` | Default model in API | ~4MB | [Model card](https://huggingface.co/cross-encoder/ms-marco-TinyBERT-L-2) |\n",
    "| `ms-marco-MiniLM-L-12-v2` | - | ~34MB | [Model card](https://huggingface.co/cross-encoder/ms-marco-MiniLM-L-12-v2) |\n",
    "| `rank-T5-flan` | Best non cross-encoder reranker | ~110MB | [Model card](https://huggingface.co/bergum/rank-T5-flan) |\n",
    "| `ms-marco-MultiBERT-L-12` | Multi-lingual, supports 100+ languages | ~150MB | [Supported languages](https://github.com/google-research/bert/blob/master/multilingual.md#list-of-languages) |\n",
    "| `ce-esci-MiniLM-L12-v2` | Fine-tuned on Amazon ESCI dataset | - | [Model card](https://huggingface.co/metarank/ce-esci-MiniLM-L12-v2) |\n",
    "| `rank_zephyr_7b_v1_full` | 4-bit-quantised GGUF | ~4GB | [Model card](https://huggingface.co/castorini/rank_zephyr_7b_v1_full) |\n",
    "| `miniReranker_arabic_v1` | - | - | [Model card](https://huggingface.co/prithivida/miniReranker_arabic_v1) |\n",
    "\n",
    "- **Cross-Endoder** models can only process **512 tokens**.\n",
    "\n",
    "- **LLM-based** models can process **8096 tokens**.\n",
    "\n",
    "The API is not compressing the documents, so if you retrieve 20 documents from database, the API will rerank all that 20 documents and **returning all back**.  \n",
    "You can just slice the top-k documents from the list (*Thats the same way that LangChain is doing*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranker = Ranker(\n",
    "    model_name=FLASHRANK_RERANK_MODEL,\n",
    "    max_length=4096,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "docs_basic = default_retriever_basic.invoke(QUERY)\n",
    "docs_title = default_retriever_title.invoke(QUERY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_list_basic = [{'id': i, 'text': doc.page_content, 'metadata': doc.metadata} for i, doc in enumerate(docs_basic)]\n",
    "docs_list_title = [{'id': i, 'text': doc.page_content, 'metadata': doc.metadata} for i, doc in enumerate(docs_title)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n = 5\n",
    "\n",
    "rerankerrequest_basic = RerankRequest(\n",
    "    query=QUERY,\n",
    "    passages=docs_list_basic,\n",
    ")\n",
    "\n",
    "rerankerrequest_title = RerankRequest(\n",
    "    query=QUERY,\n",
    "    passages=docs_list_title,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_basic = ranker.rerank(rerankerrequest_basic)\n",
    "results_title = ranker.rerank(rerankerrequest_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_basic[:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretty_output_flashrank_results(results_basic[:top_n], show_full_path=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_title[:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretty_output_flashrank_results(results_title[:top_n], show_full_path=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Project-KdxDjv4v",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
