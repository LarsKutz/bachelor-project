{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\larsk\\.virtualenvs\\Project-KdxDjv4v\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: C:\\ProgramData\\sagemaker\\sagemaker\\config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: C:\\Users\\larsk\\AppData\\Local\\sagemaker\\sagemaker\\config.yaml\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "import gradio as gr\n",
    "import os\n",
    "import time\n",
    "import urllib.parse\n",
    "from flashrank import Ranker\n",
    "from chromadb import ClientAPI\n",
    "from enum import Enum\n",
    "from langchain_core.vectorstores import VectorStoreRetriever\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain.retrievers.contextual_compression import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors.flashrank_rerank import FlashrankRerank\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_cohere import CohereRerank\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "COHERE_API_KEY = os.getenv(\"COHERE_API_KEY\")\n",
    "DB_PATH = os.getenv('DATABASE_PATH')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetrieverType(Enum):\n",
    "    DEFAULT = \"default\"\n",
    "    DEFAULT_WITH_SCORES = \"default_with_scores\"\n",
    "    MULTIQUERY = \"multiquery\"\n",
    "\n",
    "\n",
    "class RerankerType(Enum):\n",
    "    FLASHRANK = \"flashrank\"\n",
    "    COHERE = \"cohere\"\n",
    "\n",
    "\n",
    "class ChunkingStrategy(Enum):\n",
    "    \"\"\" BASIC, TITLE\n",
    "    \"\"\"\n",
    "    BASIC = \"basic\"\n",
    "    BY_TITLE = \"by_title\"\n",
    "\n",
    "\n",
    "PROMPT = ChatPromptTemplate([\n",
    "    (\"system\", \"\"\"Du bist ein Assistent einer öffentlichen Behörde und deine Aufgabe ist es, Fragen nur auf Basis des bereitgestellten Kontexts zu beantworten.\n",
    "\n",
    "- Wenn die Frage anhand des gegebenen Kontexts beantwortet werden kann, beantworte sie unter Einbeziehung relevanter Paragrafen, Gesetze oder Vorschriften, die im Kontext erwähnt werden.\n",
    "- Wenn die Frage im Kontext nicht eindeutig beantwortet werden kann oder keine ausreichenden Informationen vorliegen, gib an, dass du die Frage nicht beantworten kannst.\n",
    "- Achte besonders darauf, dass du keine Informationen hinzufügst, die nicht im Kontext enthalten sind.\n",
    "\n",
    "Am Ende deiner Antwort weise bitte darauf hin, dass du ein ChatBot bist und die Antwort unbedingt von einer qualifizierten Person überprüft werden sollte.\n",
    "\n",
    "<kontext>\n",
    "{context}\n",
    "</kontext>\"\"\"),\n",
    "    (\"human\", \"Frage: {input}\")\n",
    "])\n",
    "\n",
    "\n",
    "LLM = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\", \n",
    "    api_key=OPENAI_API_KEY, \n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "EMBEDDING_MODEL_NAME = \"text-embedding-ada-002\"\n",
    "EMBEDDINGS = OpenAIEmbeddings(\n",
    "    model=EMBEDDING_MODEL_NAME, \n",
    "    api_key=OPENAI_API_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection: collection_1500\n"
     ]
    }
   ],
   "source": [
    "# Vectorstore Basic\n",
    "\n",
    "chroma_client_basic = chromadb.PersistentClient(\n",
    "        path=os.path.join(\n",
    "            DB_PATH, \n",
    "            'Unstructured', \n",
    "            ChunkingStrategy.BASIC.value, \n",
    "            EMBEDDING_MODEL_NAME\n",
    "        )\n",
    "    )\n",
    "collection_name_basic = chroma_client_basic.list_collections()[0].name\n",
    "print(f\"Collection: {collection_name_basic}\")\n",
    "\n",
    "vectorstore_basic = Chroma(\n",
    "    collection_name=collection_name_basic,\n",
    "    embedding_function=EMBEDDINGS,\n",
    "    client=chroma_client_basic,\n",
    "    create_collection_if_not_exists=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection: collection_1800\n"
     ]
    }
   ],
   "source": [
    "# Vectorstore By Title\n",
    "\n",
    "chroma_client_by_title = chromadb.PersistentClient(\n",
    "        path=os.path.join(\n",
    "            DB_PATH, \n",
    "            'Unstructured', \n",
    "            ChunkingStrategy.BY_TITLE.value, \n",
    "            EMBEDDING_MODEL_NAME\n",
    "        )\n",
    "    )\n",
    "collection_name_by_title = chroma_client_by_title.list_collections()[0].name\n",
    "print(f\"Collection: {collection_name_by_title}\")\n",
    "\n",
    "vectorstore_by_title = Chroma(\n",
    "    collection_name=collection_name_by_title,\n",
    "    embedding_function=EMBEDDINGS,\n",
    "    client=chroma_client_by_title,\n",
    "    create_collection_if_not_exists=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectorstore(strategy):\n",
    "    s = ChunkingStrategy(strategy)\n",
    "    if s == ChunkingStrategy.BASIC:\n",
    "        return vectorstore_basic\n",
    "    elif s == ChunkingStrategy.BY_TITLE:\n",
    "        return vectorstore_by_title\n",
    "\n",
    "\n",
    "def get_retriever(strategy, retriever_type, n_retriever): \n",
    "    t = RetrieverType(retriever_type)\n",
    "    vectorstore = get_vectorstore(strategy)\n",
    "    retriever = vectorstore.as_retriever(\n",
    "        search_type='similarity',\n",
    "        search_kwargs={\n",
    "            'k': n_retriever,\n",
    "        }\n",
    "    )\n",
    "    if t == RetrieverType.DEFAULT or t == RetrieverType.DEFAULT_WITH_SCORES:\n",
    "        return retriever\n",
    "    elif t == RetrieverType.MULTIQUERY:\n",
    "        return MultiQueryRetriever.from_llm(\n",
    "            retriever=retriever,\n",
    "            llm=LLM,\n",
    "        )\n",
    "    \n",
    "\n",
    "def get_reranker(reranker_type, n_reranker):\n",
    "    t = RerankerType(reranker_type)\n",
    "    if t == RerankerType.FLASHRANK:\n",
    "        client = Ranker(\n",
    "            model_name='rank-T5-flan',\n",
    "            max_length=4096,\n",
    "        )\n",
    "        return FlashrankRerank(\n",
    "            client=client,\n",
    "            top_n=n_reranker,\n",
    "        )\n",
    "    elif t == RerankerType.COHERE:\n",
    "        return CohereRerank(\n",
    "            top_n=n_reranker,\n",
    "            model='rerank-multilingual-v3.0',\n",
    "            cohere_api_key=COHERE_API_KEY,\n",
    "        )\n",
    "\n",
    "\n",
    "def generate_answer(query, strategy, retriever_type, n_retriever, reranker_type, n_reranker):\n",
    "    retriever = get_retriever(strategy, retriever_type, n_retriever)\n",
    "    reranker = get_reranker(reranker_type[0], n_reranker) if reranker_type else None\n",
    "    \n",
    "    if reranker_type:\n",
    "        retriever = ContextualCompressionRetriever(\n",
    "            base_retriever=retriever,\n",
    "            base_compressor=reranker,\n",
    "        )\n",
    "    \n",
    "    rag_chain = create_retrieval_chain(\n",
    "        retriever=retriever,\n",
    "        combine_docs_chain=create_stuff_documents_chain(\n",
    "            llm=LLM,\n",
    "            prompt=PROMPT,\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return rag_chain.invoke({\"input\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pretty_output(answer, context):\n",
    "    return_str = f\"<span>{answer}</span><br><br><span>Kontext:</span><br><ul>\"\n",
    "    for doc in context:\n",
    "        file_path = doc.metadata[\"source\"]\n",
    "        formatted_path = file_path.replace(\"\\\\\", \"/\").replace(\"Data\", \"Source\")\n",
    "        encoded_path = urllib.parse.quote(formatted_path)\n",
    "        file_url = f\"file:///{encoded_path}\"\n",
    "        return_str += f\"<li><a href='{file_url}' target='_blank'>{os.path.basename(file_path)}</a><span> - </span><span>Seite {doc.metadata['page_number']}</span></li>\"\n",
    "    \n",
    "    return return_str + \"</ul>\"\n",
    "\n",
    "\n",
    "def generate(query, history, strategy, retriever_type, n_retriever, reranker_type, n_reranker):   # by_title default 5 [] 1\n",
    "    response = generate_answer(query, strategy, retriever_type, n_retriever, reranker_type, n_reranker)  # dict_keys(['input', 'context', 'answer'])\n",
    "    answer = pretty_output(response['answer'], response['context'])\n",
    "    for i in range(len(answer)):  \n",
    "        time.sleep(0.01)\n",
    "        yield answer[:i+1]\n",
    "\n",
    "\n",
    "chat = gr.ChatInterface(\n",
    "    theme=\"Ryouko-Yamanda65777/ryo\",\n",
    "    fn=generate,\n",
    "    type=\"messages\",\n",
    "    additional_inputs=[\n",
    "        gr.Dropdown([cs.value for cs in ChunkingStrategy], label=\"Chunking Strategy\", value=ChunkingStrategy.BY_TITLE.value),\n",
    "        gr.Dropdown([rt.value for rt in RetrieverType], label=\"Retriever Type\", value=RetrieverType.DEFAULT.value),\n",
    "        gr.Slider(minimum=1, maximum=50, step=1, value=1, label=\"Number of Chunks to retrieve with Retriever\"),\n",
    "        gr.Dropdown([rt.value for rt in RerankerType], label=\"Reranker Type\", multiselect=True, max_choices=1),\n",
    "        gr.Slider(minimum=1, maximum=10, step=1, value=1, label=\"Number of Chunks to rerank with Reranker and provide to LLM\"),\n",
    "    ],\n",
    "    additional_inputs_accordion=gr.Accordion(label=\"Advanced Options\", open=False),\n",
    "    examples=[\n",
    "        [\n",
    "            \"Ist es möglich, die Höhe einer russischen Rente umgerechnet in Euro zu erfahren?\",\n",
    "            \"by_title\",\n",
    "            \"default\",\n",
    "            5,\n",
    "            [],\n",
    "            1\n",
    "        ],\n",
    "        [\n",
    "            \"Unter welchen Voraussetzungen ist ein Stromguthaben (Haushaltsstrom) als Einkommen im Sinne des § 11 SGB II bedarfsmindernd zu berücksichtigen?\",\n",
    "            \"by_title\",\n",
    "            \"default\",\n",
    "            5,\n",
    "            [],\n",
    "            1\n",
    "        ],\n",
    "        [\n",
    "            \"Können Hilfeempfängern im Rahmen der Aufnahme einer sozialversicherungspflichtigen Beschäftigung ggfs. Fahrzeuge zur Verfügung gestellt werden, um den Arbeitsplatz zu erreichen? Fall ja, für welchen Zeitraum ist dies möglich?\",\n",
    "            \"by_title\",\n",
    "            \"default\",\n",
    "            5,\n",
    "            [],\n",
    "            1\n",
    "        ]\n",
    "    ]\n",
    ")\n",
    "\n",
    "chat.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Project-KdxDjv4v",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
